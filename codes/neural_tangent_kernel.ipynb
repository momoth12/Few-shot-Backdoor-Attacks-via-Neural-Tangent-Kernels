{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the paper implementation\n",
    "## Unfortunately takes a lot of time to run without many GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_cifar_dataset, PoisonedDataset, LabelPoisoner,PixelPoisoner\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "train_dataset = load_cifar_dataset(train=True)\n",
    "test_dataset = load_cifar_dataset(train=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract images and labels\n",
    "train_images = np.array([sample[0] for sample in train_dataset])\n",
    "train_labels = np.array([sample[1] for sample in train_dataset])\n",
    "test_images = np.array([sample[0] for sample in test_dataset])\n",
    "test_labels = np.array([sample[1] for sample in test_dataset])\n",
    "\n",
    "# Stack images and labels separately\n",
    "all_images = np.vstack([train_images, test_images])\n",
    "all_labels = np.hstack([train_labels, test_labels])\n",
    "\n",
    "# Save to .npz format for better organization\n",
    "np.savez('output/X-94-1xs-perm', images=all_images, labels=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a poisoner for specific labels if needed\n",
    "pix=PixelPoisoner()\n",
    "poisoner = LabelPoisoner(pix, target_label=1)\n",
    "poisoned_train_dataset = PoisonedDataset(train_dataset, poisoner, label=0, eps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntk_cifar import kernel_fit, x_train_c, y_train_c, x_test_c, y_test_c\n",
    "from models import WideResnet,SimpleCNN,ConvNeXt,Small_fc_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_c, y_train_c = x_train_c[:90], y_train_c[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the kernel function\n",
    "#_, _, kernel_fn = WideResnet(block_size=4, k=5, num_classes=1)\n",
    "_, _, kernel_fn =Small_fc_network()\n",
    "\n",
    "# Fit the kernel to the training data\n",
    "model, g_dd, predictor = kernel_fit(kernel_fn, x_train_c, y_train_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_fast import iter, lr, tx, params, opt_state\n",
    "import jax\n",
    "\n",
    "# Initialize JAX key and model parameters\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_epochs = 10\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    (params, opt_state, key), metrics = iter(epoch, params, opt_state, key)\n",
    "    avg_loss, avg_acc, avg_tloss, avg_tacc, avg_ploss, avg_pacc, p_pred = metrics\n",
    "    print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, Accuracy={avg_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import compute_all_reps, clf_eval\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_acc, test_loss = clf_eval(model, test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc}, Test Loss: {test_loss}\")\n",
    "\n",
    "# Extract representations from specific layers\n",
    "layer_reps = compute_all_reps(model, train_dataset, layers=[0, 2, 4])\n",
    "print(\"Layer representations extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"ntk-backdoor\", entity=\"your_entity\")\n",
    "wandb.log({\"train_loss\": avg_loss, \"train_acc\": avg_acc})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
